# -*- coding: utf-8 -*-
"""Exploratory_Data_Analysis_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fSHd3zgcg5juzNlvEuxrQfNntLGTJ1D7

<center> <h1 style="background-color:orange; color:white"><br>Exploratory Data Analysis<br></h1></center>

# `Problem Statement:`
We have used Cars dataset from kaggle  with features including make, model, year, engine, and other properties of the car used to predict its price.

## `Importing the necessary libraries`
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns #visualisation
import matplotlib.pyplot as plt #visualisation
# %matplotlib inline
sns.set(color_codes=True)
from scipy import stats
import warnings
warnings.filterwarnings("ignore")

"""## `Load the dataset into dataframe`"""

## load the csv file
df = pd.read_csv(r"C:\Users\welcome\Documents\Cars_data_.csv")

## print the head of the dataframe
df.head()

"""Now we observe the each features present in the dataset.<br>

 `Make:` The Make feature is the company name of the Car.<br>
`Model:` The Model feature is the model or different version of Car models.<br>
`Year:`  The year describes the model has been launched.<br>
`Engine Fuel Type:` It defines the Fuel type of the car model.<br>
`Engine HP:` It's say the Horsepower that refers to the power an engine produces.<br>
`Engine Cylinders:` It define the nos of cylinders in present in the engine.<br>
`Transmission Type:` It is the type of feature that describe about the car transmission type i.e Mannual or automatic.<br>
`Driven_Wheels:` The type of wheel drive.<br>
`No of doors:` It defined nos of doors present in the car.<br>
`Market Category:` This features tells about the type of car or which category the car belongs. <br>
`Vehicle Size:` It's say about the about car size.<br>
`Vehicle Style:` The feature is all about the style that belongs to car.<br>
`highway MPG:` The average a car will get while driving on an open stretch of road without stopping or starting, typically at a higher speed.<br>
`city mpg:` City MPG refers to driving with occasional stopping and braking.<br>
`Popularity:` It can refered to rating of that car or popularity of car.<br>
`MSRP:` The price of that car.

## `Check the datatypes`
"""

# Get the datatypes of each columns number of records in each column.
df.dtypes

"""## `Dropping irrevalent columns`

If we consider all columns present in the dataset then unneccessary columns will impact on the model's accuracy.<br>
Not all the columns are important to us in the given dataframe, and hence we would drop the columns that are irrevalent to us. It would reflect our model's accucary so we need to drop them. Otherwise it will affect our model.


The list cols_to_drop contains the names of the cols that are irrevalent, drop all these cols from the dataframe.


`cols_to_drop = ["Engine Fuel Type", "Market Category", "Vehicle Style", "Popularity", "Number of Doors", "Vehicle Size"]`

These features are not neccessary to obtain the model's accucary. It does not contain any relevant information in the dataset.
"""

# initialise cols_to_drop

# drop the irrevalent cols and print the head of the dataframe

df = df.drop(['Engine Fuel Type', 'Market Category', 'Vehicle Style', 'Popularity', 'Number of Doors', 'Vehicle Size'], axis=1)


# print df head
df.head(5)

"""## `Renaming the columns`

Now, Its time for renaming the feature to useful feature name. It will help to use them in model training purpose.<br>

We have already dropped the unneccesary columns, and now we are left with useful columns. One extra thing that we would do is to rename the columns such that the name clearly represents the essence of the column.

The given dict represents (in key value pair) the previous name, and the new name for the dataframe columns
"""

# rename cols
# rename_cols =

# use a pandas function to rename the current columns -
df = df.rename(columns={"Engine HP": "HP", "Engine Cylinders": "Cylinders", "Transmission Type": "Transmission", "Driven_Wheels": "Drive Mode","highway MPG": "MPG-H", "city mpg": "MPG-C", "MSRP": "Price" })

# Print the head of the dataframe
df.head(5)

"""## `Dropping the duplicate rows`

There are many rows in the dataframe which are duplicate, and hence they are just repeating the information. Its better if we remove these rows as they don't add any value to the dataframe.

For given data, we would like to see how many rows were duplicates. For this, we will count the number of rows, remove the dublicated rows, and again count the number of rows.
"""

# number of rows before removing duplicated rows
df.shape

# drop the duplicated rows

df = df.drop_duplicates()
# print head of df

df.head(5)

# Count Number of rows after deleting duplicated rows

df.shape
df.count()

"""## `Dropping the null or missing values`

Missing values are usually represented in the form of Nan or null or None in the dataset.

Finding whether we have null values in the data is by using the isnull() function.

There are many values which are missing, in pandas dataframe these values are reffered to as np.nan. We want to deal with these values beause we can't use nan values to train models. Either we can remove them to apply some strategy to replace them with other values.

To keep things simple we will be dropping nan values
"""

# check for nan values in each columns
df.isnull().sum()

"""As we can see that the HP and Cylinders have null values of 69 and 30. As these null values will impact on models' accuracy. So to avoid the impact we will drop the these values. As these values are small camparing with dataset  that will not impact any major affect on model accuracy so we will drop the values."""

# drop missing values
df = df.dropna()

# Make sure that missing values are removed
# check number of nan values in each col again
df.isnull().sum()

#Describe statistics of df

df.describe()

"""## `Removing outliers`

Sometimes a dataset can contain extreme values that are outside the range of what is expected and unlike the other data. These are called outliers and often machine learning modeling and model skill in general can be improved by understanding and even removing these outlier values.
"""

## Plot a boxplot for 'Price' column in dataset.
plt.figure(figsize=(5,3))
sns.boxplot(x=df['Price'])

"""### **`Observation:`**<br>

Here as you see that we got some values near to 1.5 and 2.0 . So these values are called outliers. Because there are away from the normal values.
Now we have detect the outliers of the feature of Price. Similarly we will checking of anothers features.
"""

## PLot a boxplot for 'HP' columns in dataset
plt.figure(figsize=(5,3))
sns.boxplot(x=df['HP'])

"""### **`Observation:`**<br>
Here boxplots show the proper distribution of of 25 percentile and 75 percentile of the feature of HP.
"""



"""print all the columns which are of int or float datatype in df.

Hint: Use loc with condition
"""

# print all the columns which are of int or float datatype in df.
integer_columns = df.select_dtypes(include=['int64']).columns
float_columns = df.select_dtypes(include=['float64']).columns

print('\nint64 columns:\n', integer_columns)
print('\nfloat64 columns:\n', float_columns)

df.loc[:, df.dtypes !=object]

"""### `Save the column names of the above output in variable list named 'l'`

"""

# save column names of the above output in variable list
l= ['Year','HP','Cylinders','MPG-H','MPG-C','Price']
print(l)

"""## **`Outliers removal techniques - IQR Method`**

**Here comes cool Fact for you!**

IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data.

- Calculate IQR  and give a suitable threshold to remove the outliers and save this new dataframe into df2.

Let us help you to decide threshold: Outliers in this case are defined as the observations that are below (Q1 âˆ’ 1.5x IQR) or above (Q3 + 1.5x IQR)
"""

## define Q1 and Q2
Q1 =df[l].quantile(0.25)
Q3 =df[l].quantile(0.75)


# # define IQR (interquantile range)
IQR =Q3-Q1
Lower_Limit=Q1-(1.5*IQR)
Upper_Limit=Q3+(1.5*IQR)


# # define df2 after removing outliers
df2 =df[l][~((df[l] < Lower_Limit)|(df[l] > Upper_Limit))]
df2

Q1 = df2.quantile(0.25)
Q3 = df2.quantile(0.75)

# # # define IQR (interquantile range)

IQR=Q3-Q1
IQR

# find the shape of df & df2
print(df.shape,df2.shape)

# find unique values and there counts in each column in df using value counts function.

col_uni_val={}
for i in df.columns:
     print ("--------------- %s ----------------" % i)
#     # code here
     col_uni_val[i] = len(df[i].unique())

import pprint
pprint.pprint(col_uni_val)

"""## `Visualising Univariate Distributions`

We will use seaborn library to visualize eye catchy univariate plots.

Do you know? you have just now already explored one univariate plot. guess which one? Yeah its box plot.

### `Histogram & Density Plots`

Histograms and density plots show the frequency of a numeric variable along the y-axis, and the value along the x-axis. The ```sns.distplot()``` function plots a density curve. Notice that this is aesthetically better than vanilla ```matplotlib```.
"""

#ploting distplot for variable HP
plt.figure(figsize=(5,3))
sns.distplot(df['HP'])

"""### **`Observation:`**
We plot the Histogram of feature HP with help of distplot in seaborn.<br>
In this graph we can see that there is max values near at 200. similary we have also the 2nd highest value near 400 and so on. <br>
It represents the overall distribution of continuous data variables.<br>

Since seaborn uses matplotlib behind the scenes, the usual matplotlib functions work well with seaborn. For example, you can use subplots to plot multiple univariate distributions.
- Hint: use matplotlib subplot function
"""

# plot all the columns present in list l together using subplot of dimention (2,3).

c=0
plt.figure(figsize=(15,10))
for i in l:
#     # code here
        df[i].plot(subplots=True, ylim=[0,100])
        plt.plot(df.index, df[i], label=0)
plt.show()

"""## `Bar Chart Plots`

Plot a histogram depicting the make in X axis and number of cars in y axis. <br>
"""

plt.figure(figsize = (12,8))

# use nlargest and then .plot to get bar plot like below output
# Plot Title, X & Y label
large5 = df.nlargest(5, "Price")
sns.histplot(df['Make'], bins=30, kde=True, color='lightgreen', edgecolor='red')

# Adding labels and title
plt.xlabel('Make')
plt.xticks(rotation=90)
plt.ylabel('Number of Cars')
plt.title('Make vs Number of cars')

# display
#large5
plt.show()

"""### **`Observation:`**
In this plot we can see that we have plot the bar plot with the cars model and nos. of cars.

### `Count Plot`
A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable.

Plot a countplot for a variable Transmission vertically with hue as Drive mode
"""

plt.figure(figsize=(15,5))

# plot countplot on transmission and drive mode

sns.countplot(x ='Transmission', data = df)
plt.xticks(rotation=45)
# Show the plot
plt.show()

"""### **`Observation:`**
In this count plot, We have plot the feature of Transmission with help of hue.<br>
We can see that the the nos of count and the transmission type and automated manual is plotted. Drive mode as been given with help of hue.<br>

# `Visualising Bivariate Distributions`


Bivariate distributions are simply two univariate distributions plotted on x and y axes respectively. They help you observe the relationship between the two variables.

## `Scatter Plots`
Scatterplots are used to find the correlation between two continuos variables.

Using scatterplot find the correlation between 'HP' and 'Price' column of the data.
"""

## Your code here -
fig, ax = plt.subplots(figsize=(10,6))

# plot scatterplot on hp and price


sns.scatterplot(x="HP",
                    y="Price",
                    data=df)

"""### **`Observation:`**<br>
It is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data.<br>
We have plot the scatter plot with x axis as HP and y axis as Price.<br>
The data points between the features should be same either wise it give errors.<br>

## `Plotting Aggregated Values across Categories`


### `Bar Plots - Mean, Median and Count Plots`



Bar plots are used to **display aggregated values** of a variable, rather than entire distributions. This is especially useful when you have a lot of data which is difficult to visualise in a single figure.

For example, say you want to visualise and *compare the Price across Cylinders*. The ```sns.barplot()``` function can be used to do that.
"""

# bar plot with default statistic=mean between Cylinder and Price

sns.barplot(x = 'Cylinders', y = 'Price', data = df)

# Show the plot
plt.show()

"""### **`Observation:`**<br>
By default, seaborn plots the mean value across categories, though you can plot the count, median, sum etc.<br>
Also, barplot computes and shows the confidence interval of the mean as well.

## `When you want to visualise having a large number of categories, it is helpful to plot the categories across the y-axis.`

### `Let's now drill down into Transmission sub categories.`
"""

# Plotting categorical variable Transmission across the y-axis

sns.barplot(x = 'Cylinders', y = 'Transmission', data = df)

# Show the plot
plt.show()

"""These plots looks beutiful isn't it? In Data Analyst life such charts are there unavoidable friend.:)

# `Multivariate Plots`

## `Heatmaps`


A heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information

Using heatmaps plot the correlation between the features present in the dataset.
"""

#find the correlation of features of the data
corr =df[l].corr()

# print corr
corr

# Using the correlated df, plot the heatmap
# set cmap = 'BrBG', annot = True - to get the same graph as shown below
# set size of graph = (12,8)
plt.figure(figsize = (12,8))
sns.heatmap(corr, cmap = 'BrBG')
plt.show()

"""### **`Observation:`**<br>
A heatmap contains values representing various shades of the same colour for each value to be plotted. Usually the darker shades of the chart represent higher values than the lighter shade. For a very different value a completely different colour can also be used.


The above heatmap plot shows correlation between various variables in the colored scale of -1 to 1.

"""

